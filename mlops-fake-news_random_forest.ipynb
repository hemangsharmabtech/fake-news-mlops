{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8165591,"sourceType":"datasetVersion","datasetId":4831777}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-01T16:06:55.639279Z","iopub.execute_input":"2025-10-01T16:06:55.639665Z","iopub.status.idle":"2025-10-01T16:06:55.648629Z","shell.execute_reply.started":"2025-10-01T16:06:55.639641Z","shell.execute_reply":"2025-10-01T16:06:55.647758Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/fake-and-real-news-dataset/True.csv\n/kaggle/input/fake-and-real-news-dataset/Fake.csv\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport joblib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T16:06:55.649935Z","iopub.execute_input":"2025-10-01T16:06:55.650208Z","iopub.status.idle":"2025-10-01T16:06:55.723562Z","shell.execute_reply.started":"2025-10-01T16:06:55.650186Z","shell.execute_reply":"2025-10-01T16:06:55.722624Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# --- 1. Load Data and Define Sample Size ---\nSAMPLE_SIZE_PER_CLASS = 10000\nTOTAL_SAMPLE_SIZE = SAMPLE_SIZE_PER_CLASS * 2\n\ntry:\n    # Load the datasets, reading only the required number of rows\n    df_fake = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv').head(SAMPLE_SIZE_PER_CLASS)\n    df_true = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv').head(SAMPLE_SIZE_PER_CLASS)\n    \n    print(f\"Loaded {len(df_fake)} fake samples and {len(df_true)} true samples.\")\nexcept FileNotFoundError:\n    print(\"Error: Make sure 'Fake.csv' and 'True.csv' are in the same directory as your script.\")\n    exit()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T16:06:55.725537Z","iopub.execute_input":"2025-10-01T16:06:55.726423Z","iopub.status.idle":"2025-10-01T16:06:57.241712Z","shell.execute_reply.started":"2025-10-01T16:06:55.726383Z","shell.execute_reply":"2025-10-01T16:06:57.240716Z"}},"outputs":[{"name":"stdout","text":"Loaded 10000 fake samples and 10000 true samples.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# --- 2. Labeling and Combination ---\n# Add a 'label' column to each DataFrame (0 for fake, 1 for true)\ndf_fake['label'] = 0\ndf_true['label'] = 1\n\n# Combine the datasets\ndf = pd.concat([df_fake, df_true], ignore_index=True)\n\n# Shuffle the combined data to mix fake and true news\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(f\"Total dataset size after sampling and combining: {len(df)} rows.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T16:06:57.242766Z","iopub.execute_input":"2025-10-01T16:06:57.243049Z","iopub.status.idle":"2025-10-01T16:06:57.267040Z","shell.execute_reply.started":"2025-10-01T16:06:57.243027Z","shell.execute_reply":"2025-10-01T16:06:57.266042Z"}},"outputs":[{"name":"stdout","text":"Total dataset size after sampling and combining: 20000 rows.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# --- 3. Text Preprocessing and Feature Engineering (Addressing Leakage) ---\n# ✅ CHANGE: Explicitly drop 'subject' and 'date' to prevent trivial data leakage.\n# The model should classify based on content style, not source-based metadata.\ndf = df.drop(columns=['subject', 'date'], errors='ignore') \n\n# Combine 'title' and 'text' into a single feature for classification\ndf['content'] = df['title'] + ' ' + df['text']\n\n# Initialize NLTK stopwords (download if necessary)\ntry:\n    stop_words = set(stopwords.words('english'))\nexcept LookupError:\n    nltk.download('stopwords')\n    stop_words = set(stopwords.words('english'))\n\ndef preprocess_text(text):\n    \"\"\"Clean the text: remove non-alphanumeric, lowercase, and remove stopwords.\"\"\"\n    if isinstance(text, str):\n        # Remove non-alphanumeric characters (keep spaces)\n        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n        text = text.lower()\n        # Remove stopwords and join\n        text = ' '.join([word for word in text.split() if word not in stop_words])\n    return text\n\n# Apply preprocessing\ndf['content'] = df['content'].apply(preprocess_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T16:06:57.269712Z","iopub.execute_input":"2025-10-01T16:06:57.270131Z","iopub.status.idle":"2025-10-01T16:06:59.973708Z","shell.execute_reply.started":"2025-10-01T16:06:57.270094Z","shell.execute_reply":"2025-10-01T16:06:59.972864Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# --- 4. Split Data and Vectorize ---\nX = df['content']\ny = df['label']\n\n# Split the 5000 samples into training and testing sets (e.g., 80/20 split)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nprint(f\"\\nTraining set size: {len(X_train)} samples\")\nprint(f\"Testing set size: {len(X_test)} samples\")\n\n# Initialize and fit TF-IDF Vectorizer on the training data\n# TF-IDF converts text into numerical feature vectors\ntfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\nX_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)\n\nprint(f\"Number of features (TF-IDF vector length): {X_train_tfidf.shape[1]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T16:06:59.974983Z","iopub.execute_input":"2025-10-01T16:06:59.975258Z","iopub.status.idle":"2025-10-01T16:07:19.778646Z","shell.execute_reply.started":"2025-10-01T16:06:59.975236Z","shell.execute_reply":"2025-10-01T16:07:19.777380Z"}},"outputs":[{"name":"stdout","text":"\nTraining set size: 16000 samples\nTesting set size: 4000 samples\nNumber of features (TF-IDF vector length): 5000\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# --- 5. Train Random Forest Model ---\nprint(\"\\nStarting Random Forest training...\")\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\nrf_model.fit(X_train_tfidf, y_train)\nprint(\"Training complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T16:07:19.779722Z","iopub.execute_input":"2025-10-01T16:07:19.780076Z","iopub.status.idle":"2025-10-01T16:07:24.942515Z","shell.execute_reply.started":"2025-10-01T16:07:19.780045Z","shell.execute_reply":"2025-10-01T16:07:24.941701Z"}},"outputs":[{"name":"stdout","text":"\nStarting Random Forest training...\nTraining complete.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# --- 6. Model Evaluation ---\n# Predict on the test set\ny_pred = rf_model.predict(X_test_tfidf)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\n--- Model Evaluation (Random Forest on {TOTAL_SAMPLE_SIZE} samples) ---\")\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['Fake (0)', 'True (1)']))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T16:07:24.943398Z","iopub.execute_input":"2025-10-01T16:07:24.943618Z","iopub.status.idle":"2025-10-01T16:07:25.080158Z","shell.execute_reply.started":"2025-10-01T16:07:24.943598Z","shell.execute_reply":"2025-10-01T16:07:25.079166Z"}},"outputs":[{"name":"stdout","text":"\n--- Model Evaluation (Random Forest on 20000 samples) ---\nAccuracy: 0.9980\n\nClassification Report:\n              precision    recall  f1-score   support\n\n    Fake (0)       1.00      1.00      1.00      2000\n    True (1)       1.00      1.00      1.00      2000\n\n    accuracy                           1.00      4000\n   macro avg       1.00      1.00      1.00      4000\nweighted avg       1.00      1.00      1.00      4000\n\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# --- 7. Save the Model and Vectorizer (MLOPS ARTIFACTS) ---\nMODEL_FILENAME = 'rf_fake_news_model.joblib'\nVECTORIZER_FILENAME = 'tfidf_vectorizer.joblib'\n\n# ✅ CHANGE: Save the trained model \njoblib.dump(rf_model, MODEL_FILENAME)\nprint(f\"\\nModel successfully saved to: {MODEL_FILENAME}\")\n\n# ✅ CHANGE: Save the fitted vectorizer (crucial for preprocessing new data)\njoblib.dump(tfidf_vectorizer, VECTORIZER_FILENAME)\nprint(f\"Vectorizer successfully saved to: {VECTORIZER_FILENAME}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T16:07:25.081241Z","iopub.execute_input":"2025-10-01T16:07:25.081552Z","iopub.status.idle":"2025-10-01T16:07:32.056547Z","shell.execute_reply.started":"2025-10-01T16:07:25.081521Z","shell.execute_reply":"2025-10-01T16:07:32.055541Z"}},"outputs":[{"name":"stdout","text":"\nModel successfully saved to: rf_fake_news_model.joblib\nVectorizer successfully saved to: tfidf_vectorizer.joblib\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# --- OPTIONAL: Demonstrate Loading the Model ---\n# This is what your MLOps deployment service would do\n\nprint(\"\\n--- Demonstration of Loading and Predicting ---\")\n# Load the model and vectorizer\nloaded_model = joblib.load(MODEL_FILENAME)\nloaded_vectorizer = joblib.load(VECTORIZER_FILENAME)\n\n# Example new data (a fake news article and a real-sounding headline)\nnew_articles = [\n    \"BREAKING NEWS: The President was seen flying a gigantic purple dragon over the White House this morning, witnesses confirm.\",\n    \"United Nations Security Council votes unanimously to impose new sanctions on the country of North Korea after a recent missile launch, according to reports from Reuters.\",\n]\n\n# Preprocess and vectorize the new data using the loaded vectorizer\nnew_articles_processed = [preprocess_text(text) for text in new_articles]\nnew_articles_vectorized = loaded_vectorizer.transform(new_articles_processed)\n\n# Make predictions\npredictions = loaded_model.predict(new_articles_vectorized)\nprediction_labels = ['Fake' if p == 0 else 'True' for p in predictions]\n\nfor article, label in zip(new_articles, prediction_labels):\n    print(f\"Article: '{article[:70]}...' -> Prediction: {label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T16:07:32.057663Z","iopub.execute_input":"2025-10-01T16:07:32.057971Z","iopub.status.idle":"2025-10-01T16:07:35.504402Z","shell.execute_reply.started":"2025-10-01T16:07:32.057946Z","shell.execute_reply":"2025-10-01T16:07:35.503355Z"}},"outputs":[{"name":"stdout","text":"\n--- Demonstration of Loading and Predicting ---\nArticle: 'BREAKING NEWS: The President was seen flying a gigantic purple dragon ...' -> Prediction: Fake\nArticle: 'United Nations Security Council votes unanimously to impose new sancti...' -> Prediction: True\n","output_type":"stream"}],"execution_count":34}]}